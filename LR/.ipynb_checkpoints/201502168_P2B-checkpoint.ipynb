{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "colab_type": "code",
    "id": "j8GqYHOkcJ42",
    "outputId": "35041801-551d-43bb-a8ee-b5504dab0005"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gensim in ./py3_env/lib/python3.5/site-packages (3.6.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in ./py3_env/lib/python3.5/site-packages (from gensim) (1.15.2)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in ./py3_env/lib/python3.5/site-packages (from gensim) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in ./py3_env/lib/python3.5/site-packages (from gensim) (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.2.1 in ./py3_env/lib/python3.5/site-packages (from gensim) (1.7.1)\n",
      "Requirement already satisfied, skipping upgrade: boto>=2.32 in ./py3_env/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
      "Requirement already satisfied, skipping upgrade: bz2file in ./py3_env/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim) (0.98)\n",
      "Requirement already satisfied, skipping upgrade: requests in ./py3_env/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim) (2.19.1)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in ./py3_env/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim) (1.9.19)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in ./py3_env/lib/python3.5/site-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.8,>=2.5 in ./py3_env/lib/python3.5/site-packages (from requests->smart-open>=1.2.1->gensim) (2.7)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.24,>=1.21.1 in ./py3_env/lib/python3.5/site-packages (from requests->smart-open>=1.2.1->gensim) (1.23)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in ./py3_env/lib/python3.5/site-packages (from requests->smart-open>=1.2.1->gensim) (2018.8.24)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.2.0,>=0.1.10 in ./py3_env/lib/python3.5/site-packages (from boto3->smart-open>=1.2.1->gensim) (0.1.13)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.13.0,>=1.12.19 in ./py3_env/lib/python3.5/site-packages (from boto3->smart-open>=1.2.1->gensim) (1.12.19)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in ./py3_env/lib/python3.5/site-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.3)\n",
      "Requirement already satisfied, skipping upgrade: docutils>=0.10 in ./py3_env/lib/python3.5/site-packages (from botocore<1.13.0,>=1.12.19->boto3->smart-open>=1.2.1->gensim) (0.14)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in ./py3_env/lib/python3.5/site-packages (from botocore<1.13.0,>=1.12.19->boto3->smart-open>=1.2.1->gensim) (2.7.3)\n",
      "Requirement already satisfied: nltk in ./py3_env/lib/python3.5/site-packages (3.3)\n",
      "Requirement already satisfied: six in ./py3_env/lib/python3.5/site-packages (from nltk) (1.11.0)\n",
      "Requirement already satisfied: Cython in ./py3_env/lib/python3.5/site-packages (0.28.5)\n",
      "Requirement already satisfied: fasttext in ./py3_env/lib/python3.5/site-packages (0.8.3)\n",
      "Requirement already satisfied: numpy>=1 in ./py3_env/lib/python3.5/site-packages (from fasttext) (1.15.2)\n",
      "Requirement already satisfied: future in ./py3_env/lib/python3.5/site-packages (from fasttext) (0.16.0)\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /home/priyansh.agrawal/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "/home/priyansh.agrawal/saved_models\n"
     ]
    }
   ],
   "source": [
    "#Just like in P2(a), perform POS Tagging on the Brown corpus. (Like before, train your Logistic Regression model on the\n",
    "#tagged corpus, and test on the untagged one). \n",
    "#Use one vs all logistic regression to perform this exercise. \n",
    "#Essentially, given a word, try to classify it with classifiers trained for all pos tags and get most probable one.\n",
    "#Do NOT use any ML libraries like scipy for coding up the logistic regression. NLTK maybe allowed, but only for \n",
    "#getting corpus.\n",
    "!pip install -U gensim\n",
    "!pip install nltk\n",
    "!pip install Cython\n",
    "!pip install fasttext \n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import gensim.downloader as api\n",
    "from gensim.models import FastText\n",
    "model = FastText.load_fasttext_format(\"wiki.en.bin\")\n",
    "script_path=os.path.dirname(os.path.realpath('__file__'))+\"/saved_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "whtM2ADGcJ45"
   },
   "outputs": [],
   "source": [
    "\n",
    "#one way is using class (optional), advantage being you can create multiple instances of class and train for each pos tag\n",
    "\n",
    "class logistic_regression:\n",
    "    def __init__(self):\n",
    "        self.W = None # set up the weight matrix \n",
    "\n",
    "    def train(self, X, y,learning_rate=1e-4,reg = 1e3,num_iters=1000,softmax=False):\n",
    "        dim, num_train = X.shape\n",
    "        num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n",
    "        if self.W is None:\n",
    "            # initialize the weights with small values\n",
    "            if num_classes == 2: # just need weights for one class\n",
    "                self.W = np.random.randn(1, dim) * 0.001\n",
    "            else: # weigths for each class\n",
    "                self.W = np.random.randn(num_classes, dim) * 0.001  \n",
    "        losses_history = [] #(list) of losses at each training iteration\n",
    "        for i in xrange(num_iters):\n",
    "            loss, grad = self.loss_grad(X, y, reg, softmax) # grad => [K x D]\n",
    "            losses_history.append(loss)\n",
    "            # update weights\n",
    "            self.W -= learning_rate * grad # [K x D]\n",
    "            ite = num_iters/10\n",
    "            if (i % ite == 0):\n",
    "                print 'iteration %d/%d: loss %f' % (i, num_iters, loss)\n",
    "        return losses_history\n",
    "\n",
    "    def predict(self, X,softmax=False):\n",
    "        pred_ys = np.zeros(X.shape[1])\n",
    "        f_x_mat = self.W.dot(X)\n",
    "        if not softmax:\n",
    "            pred_ys = f_x_mat.squeeze() >=0\n",
    "        else: # use softamx for multiclassification\n",
    "            pred_ys = np.argmax(f_x_mat, axis=0) #(N, ) 1-dimension array of y for N sampels\n",
    "        # normalized score\n",
    "        h_x_mat = 1.0 / (1.0 + np.exp(-f_x_mat)) # [1, N]\n",
    "        h_x_mat = h_x_mat.squeeze() #Normalized scores\n",
    "        return pred_ys, h_x_mat\n",
    "    \n",
    "    def loss_grad_softmax(self,X, y,reg):\n",
    "        \"\"\" Compute the loss and gradients using softmax with vectorized version\"\"\"\n",
    "        loss = 0 \n",
    "        grad = np.zeros_like(self.W)\n",
    "        dim, num_train = X.shape\n",
    "        scores = self.W.dot(X) # [K, N]\n",
    "        # Shift scores so that the highest value is 0\n",
    "        scores -= np.max(scores)\n",
    "        scores_exp = np.exp(scores)\n",
    "        correct_scores_exp = scores_exp[y, xrange(num_train)] # [N, ]\n",
    "        scores_exp_sum = np.sum(scores_exp, axis=0) # [N, ]\n",
    "        loss = -np.sum(np.log(correct_scores_exp / scores_exp_sum))\n",
    "        loss /= num_train\n",
    "        loss+= 0.5 * reg * np.sum(self.W * self.W)\n",
    "        scores_exp_normalized = scores_exp / scores_exp_sum\n",
    "        # deal with the correct class\n",
    "        scores_exp_normalized[y, xrange(num_train)] -= 1 # [K, N]\n",
    "        grad = scores_exp_normalized.dot(X.T)\n",
    "        grad /= num_train\n",
    "        grad += reg * self.W\n",
    "        return loss, grad\n",
    "\n",
    "    def loss_grad_logistic(self,X, y, reg):\n",
    "        \"\"\"Compute the loss and gradients with weights, vectorized version\"\"\"\n",
    "        dim, num_train = X.shape\n",
    "        loss = 0\n",
    "        grad = np.zeros_like(self.W) # [1, D]\n",
    "        f_x_mat = self.W.dot(X) # [1, D] * [D, N]\n",
    "        h_x_mat = 1.0 / (1.0 + np.exp(-f_x_mat)) # [1, N]\n",
    "        loss = np.sum(y * np.log(h_x_mat) + (1 - y) * np.log(1 - h_x_mat))\n",
    "        loss = -1.0 / num_train * loss + 0.5 * reg * np.sum(self.W * self.W)\n",
    "        grad = (h_x_mat - y).dot(X.T) # [1, D]\n",
    "        grad = 1.0 / num_train * grad + reg *self.W\n",
    "        return loss, grad\n",
    "\n",
    "    def loss_grad(self, X, y, reg,softmax):\n",
    "        #loss: (float)\n",
    "        #grad: (array) with respect to self.W\n",
    "        if not softmax:\n",
    "            return self.loss_grad_logistic(X, y, reg)\n",
    "        else:\n",
    "            return self.loss_grad_softmax(X, y, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NqY1UppmcJ48"
   },
   "outputs": [],
   "source": [
    "def predict_one_vs_all(logistic_classifiers, X, num_classes):\n",
    "    scores = np.zeros((num_classes, X.shape[1]))\n",
    "    for i in xrange(num_classes):\n",
    "        logistic = logistic_classifiers[i]\n",
    "        scores[i,:] = logistic.predict(X)[1]\n",
    "    pred_X = np.argmax(scores, axis=0)\n",
    "    return pred_X\n",
    "\n",
    "def train_one_vs_all(X_train,y_train,learning_rate,reg,num_iters):\n",
    "    logistic_classifiers = []\n",
    "    num_classes = np.max(y_train) + 1\n",
    "    losses = []\n",
    "    for i in xrange(num_classes):\n",
    "        print '\\nThe %d/%dth logistic classifier training...' % (i+1, num_classes)\n",
    "        y_train_logistic = deepcopy(y_train)\n",
    "        for j in range(len(y_train_logistic)):\n",
    "            if y_train_logistic[j]!=i:\n",
    "                y_train_logistic[j]=0\n",
    "            else:\n",
    "                y_train_logistic[j]=1\n",
    "        #y_train_logistic[y_train_logistic!=i]=0\n",
    "        #y_train_logistic[y_train_logistic==i]=1\n",
    "        logistic = logistic_regression()\n",
    "        loss = logistic.train(X_train, y_train_logistic,learning_rate,reg,num_iters)\n",
    "        losses.append(loss)\n",
    "        logistic_classifiers.append(logistic)\n",
    "    return logistic_classifiers    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qRdVMdHzcJ4-"
   },
   "source": [
    "Task 2 : Predict tag sequence and get accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LkzyjHNTcJ4_"
   },
   "outputs": [],
   "source": [
    "def pred_tag_sequence(sentence,ft_model,model,class_map,model_name=\"logi\"):\n",
    "    # Given a sentence, Get sequence of tags for it by getting most prefered tag for each word given  \n",
    "    # Feel free to add helper functions more features ( like say trigram w1w2w3 as features for w2)\n",
    "    ##assuming sentence is list of tokens\n",
    "    num_classes = len(class_map)\n",
    "    X = word_to_vec(ft_model,sentence[0])\n",
    "    for i in range(1,len(sentence)):\n",
    "        X=np.vstack((X,word_to_vec(ft_model,sentence[i])))\n",
    "    mean_image = np.mean(X, axis = 0)\n",
    "    X -= mean_image\n",
    "    X = np.hstack([X, np.ones((X.shape[0], 1))]).T\n",
    "    if model_name!=\"logi\":\n",
    "        result= model.predict(X,softmax=True)\n",
    "        y_pred = result[0]\n",
    "        score = result[1]\n",
    "    else:\n",
    "        y_pred= predict_one_vs_all(model, X, num_classes)\n",
    "    print_tags(class_map,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 894
    },
    "colab_type": "code",
    "id": "xS0cpQo-kAJ_",
    "outputId": "456ef622-7f99-43e5-a830-a4a5fa08ae55"
   },
   "outputs": [],
   "source": [
    "def print_tags(class_map,y_pred):\n",
    "    tags=[]\n",
    "    for ind in y_pred:\n",
    "        for tag,idx in class_map.iteritems():\n",
    "            if idx==ind:\n",
    "                tags.append(tag)\n",
    "                break\n",
    "    print \"Predicted Tags: \",tags            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tCejhJsjkBzd"
   },
   "outputs": [],
   "source": [
    "def word_to_vec(model,word):\n",
    "    return model.wv[word]\n",
    "\n",
    "def extract_data(model,class_map):\n",
    "    y=np.array([],dtype=int)\n",
    "    co_vocab=[]\n",
    "    fl=1\n",
    "    for sent in brown.tagged_sents():\n",
    "        for (word,tag) in sent:\n",
    "            if not fl:\n",
    "                if (word,tag[:2]) not in co_vocab:\n",
    "                    co_vocab.append((word,tag[:2]))\n",
    "                    try:\n",
    "                        x_i = word_to_vec(model,word)\n",
    "                    except KeyError:\n",
    "                        print \"Ignoring OOV words...\"\n",
    "                        print word\n",
    "                        continue\n",
    "                    temp = np.vstack((temp,x_i))\n",
    "                    y = np.append(y,int(class_map[tag[:2]]))\n",
    "            else:\n",
    "                temp = word_to_vec(model,word)\n",
    "                y=np.append(y,int(class_map[tag[:2]]))\n",
    "                fl=0\n",
    "    X=temp.T\n",
    "    return X,y\n",
    "\n",
    "def preprocessing(X_train,X_val,X_test):\n",
    "    # Normalize the data: subtract the mean image\n",
    "    X_train=X_train.T\n",
    "    X_test=X_test.T\n",
    "    X_val=X_val.T\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    \n",
    "    # Add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))]).T\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))]).T\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))]).T\n",
    "    return X_train,X_val,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 948
    },
    "colab_type": "code",
    "id": "4Lur_4FeiGAA",
    "outputId": "fc7c5bd2-952d-45d7-b58c-57376febbdd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"X,y = extract_data(model,class_map)\\ntrain_test_split=60\\ntotal_sent=len(brown.tagged_sents())\\ntrain_sent = int(train_test_split*total_sent/100)\\nX_train,y_train = X[0:X.shape[0],0:train_sent],y[0:train_sent]\\nX_test,y_test = X[0:X.shape[0],train_sent:train_sent + int((total_sent-train_sent)/2)],y[train_sent:train_sent + int((total_sent-train_sent)/2)]\\nX_val,y_val = X[0:X.shape[0],train_sent + int((total_sent-train_sent)/2):total_sent],y[train_sent + int((total_sent-train_sent)/2):total_sent]\\nX_train,X_val,X_test = preprocessing(X_train,X_val,X_test)\\nprint 'Train data shape: ', X_train.shape\\nprint 'Train labels shape: ', y_train.shape\\nprint 'Validation data shape: ', X_val.shape\\nprint 'Validation labels shape: ', y_val.shape\\nprint 'Test data shape: ', X_test.shape\\nprint 'Test labels shape: ', y_test.shape\\ndata=[X_train,y_train,X_val,y_val,X_test,y_test]\\nsave_data(script_path,data)\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = [str(tag[:2]) for sent in brown.tagged_sents() for (word,tag) in sent]\n",
    "classes = set(classes)\n",
    "l=0\n",
    "r=len(classes)\n",
    "class_map = dict(zip(classes,xrange(l,r)))\n",
    "'''X,y = extract_data(model,class_map)\n",
    "train_test_split=60\n",
    "total_sent=len(brown.tagged_sents())\n",
    "train_sent = int(train_test_split*total_sent/100)\n",
    "X_train,y_train = X[0:X.shape[0],0:train_sent],y[0:train_sent]\n",
    "X_test,y_test = X[0:X.shape[0],train_sent:train_sent + int((total_sent-train_sent)/2)],y[train_sent:train_sent + int((total_sent-train_sent)/2)]\n",
    "X_val,y_val = X[0:X.shape[0],train_sent + int((total_sent-train_sent)/2):total_sent],y[train_sent + int((total_sent-train_sent)/2):total_sent]\n",
    "X_train,X_val,X_test = preprocessing(X_train,X_val,X_test)\n",
    "print 'Train data shape: ', X_train.shape\n",
    "print 'Train labels shape: ', y_train.shape\n",
    "print 'Validation data shape: ', X_val.shape\n",
    "print 'Validation labels shape: ', y_val.shape\n",
    "print 'Test data shape: ', X_test.shape\n",
    "print 'Test labels shape: ', y_test.shape\n",
    "data=[X_train,y_train,X_val,y_val,X_test,y_test]\n",
    "save_data(script_path,data)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(script_path,data):\n",
    "    with open(script_path+\"/X_train.pickle\",'wb') as fl:\n",
    "        pickle.dump(data[0],fl)\n",
    "    fl.close()\n",
    "    with open(script_path+\"/y_train.pickle\",'wb') as fl:\n",
    "        pickle.dump(data[1],fl)\n",
    "    fl.close()\n",
    "    with open(script_path+\"/X_val.pickle\",'wb') as fl:\n",
    "        pickle.dump(data[2],fl)\n",
    "    fl.close()\n",
    "    with open(script_path+\"/y_val.pickle\",'wb') as fl:\n",
    "        pickle.dump(data[3],fl)\n",
    "    fl.close()\n",
    "    with open(script_path+\"/X_test.pickle\",'wb') as fl:\n",
    "        pickle.dump(data[4],fl)\n",
    "    fl.close()\n",
    "    with open(script_path+\"/y_test.pickle\",'wb') as fl:\n",
    "        pickle.dump(data[5],fl)\n",
    "    fl.close()\n",
    "    \n",
    "def load_data(script_path):\n",
    "    fl = open(script_path+\"/X_train.pickle\",'rb')\n",
    "    X_train = pickle.load(fl)\n",
    "    fl.close()\n",
    "    fl = open(script_path+\"/y_train.pickle\",'rb')\n",
    "    y_train = pickle.load(fl)\n",
    "    fl.close()\n",
    "    fl = open(script_path+\"/X_val.pickle\",'rb')\n",
    "    X_val = pickle.load(fl)\n",
    "    fl.close()\n",
    "    fl = open(script_path+\"/y_val.pickle\",'rb')\n",
    "    y_val = pickle.load(fl)\n",
    "    fl.close()\n",
    "    fl = open(script_path+\"/X_test.pickle\",'rb')\n",
    "    X_test = pickle.load(fl)\n",
    "    fl.close()\n",
    "    fl = open(script_path+\"/y_test.pickle\",'rb')\n",
    "    y_test = pickle.load(fl)\n",
    "    fl.close()\n",
    "    return X_train,y_train,X_val,y_val,X_test,y_test\n",
    "\n",
    "def save_model(script_path,model_name,model):\n",
    "    with open(script_path+\"/\"+model_name+\".pickle\",'wb') as fl:\n",
    "        pickle.dump(model,fl)\n",
    "    fl.close() \n",
    "def load_model(script_path,model_name):\n",
    "    fl = open(script_path+\"/\"+model_name+\".pickle\",'rb')\n",
    "    model = pickle.load(fl)\n",
    "    fl.close()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 894
    },
    "colab_type": "code",
    "id": "LfwpzMM_dDOg",
    "outputId": "f90c04b0-37b7-44bc-9efc-9ebb66e85f69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (301, 34404)\n",
      "Train labels shape:  (34404,)\n",
      "Validation data shape:  (301, 11468)\n",
      "Validation labels shape:  (11468,)\n",
      "Test data shape:  (301, 11468)\n",
      "Test labels shape:  (11468,)\n"
     ]
    }
   ],
   "source": [
    "X_train,y_train,X_val,y_val,X_test,y_test = load_data(script_path)\n",
    "print 'Train data shape: ', X_train.shape\n",
    "print 'Train labels shape: ', y_train.shape\n",
    "print 'Validation data shape: ', X_val.shape\n",
    "print 'Validation labels shape: ', y_val.shape\n",
    "print 'Test data shape: ', X_test.shape\n",
    "print 'Test labels shape: ', y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_with_softmax(X_test,y_test,best_softmax):\n",
    "    y_test_predict_result = best_softmax.predict(X_test,softmax=True)\n",
    "    y_test_predict = y_test_predict_result[0]\n",
    "    test_accuracy = np.mean(y_test == y_test_predict)\n",
    "    print 'The test accuracy is: %f' % test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning_hyperparams(X_train,y_train,X_val,y_val,learning_rates=[1e-6, 1e-8],regularization_strengths=[1e3, 1e5],interval=5,epochs=1000):\n",
    "    best_eta=0\n",
    "    best_lambda=0\n",
    "    best_val = -1\n",
    "    best_softmax_model = None\n",
    "    # Choose the best hyperparameters by tuning on the validation set\n",
    "    i = 0\n",
    "    for learning_rate in np.linspace(learning_rates[0], learning_rates[1], num=interval):\n",
    "        i += 1\n",
    "        print 'The current iteration is %d/%d %f' % (i, interval,learning_rate)\n",
    "        for reg in np.linspace(regularization_strengths[0], regularization_strengths[1], num=interval):\n",
    "            print 'Regularization strength at this point ... %f' % (reg)\n",
    "            sftmax = logistic_regression()\n",
    "            loss = sftmax.train(X_train, y_train,learning_rate=learning_rate,reg = reg, num_iters=epochs,softmax=True)\n",
    "            y_val_pred = sftmax.predict(X_val,softmax=True)[0]\n",
    "            val_accuracy = np.mean(y_val == y_val_pred)\n",
    "            #results[(learning_rate, reg)] = val_accuracy\n",
    "            if val_accuracy > best_val:\n",
    "                best_val = val_accuracy\n",
    "                best_eta = learning_rate\n",
    "                best_lambda = reg\n",
    "                best_softmax_model = sftmax\n",
    "            else:\n",
    "                pass\n",
    "    return best_eta,best_lambda,best_softmax_model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'learning_rates=[1e-6, 1e-8]\\nregularization_strengths=[1e3, 1e5]\\nnum_iters=1000\\neta,lambda_,soft_model = tuning_hyperparams(X_train,y_train,X_val,y_val,learning_rates=learning_rates,regularization_strengths=regularization_strengths,epochs=num_iters)'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''learning_rates=[1e-6, 1e-8]\n",
    "regularization_strengths=[1e3, 1e5]\n",
    "num_iters=1000\n",
    "eta,lambda_,soft_model = tuning_hyperparams(X_train,y_train,X_val,y_val,learning_rates=learning_rates,regularization_strengths=regularization_strengths,epochs=num_iters)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy is: 0.395274\n"
     ]
    }
   ],
   "source": [
    "eta=2.5749999999999986e-07\n",
    "lambda_ = 50500.0\n",
    "#print \"Best learning_rate: \",eta\n",
    "#print \"Best regularization_rate: \",lambda_\n",
    "#print \"Saving in :\",script_path        \n",
    "#save_model(script_path,\"softmax\",soft_model)    \n",
    "soft_model = load_model(script_path,\"softmax\")\n",
    "test_with_softmax(X_test,y_test,soft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test datast accuracy: 0.395535\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(classes)\n",
    "#logistic_classifiers = train_one_vs_all(X_train,y_train,learning_rate=eta,reg=lambda_,num_iters=1000)\n",
    "#print \"Saving in :\",script_path\n",
    "#save_model(script_path,\"logis\",logistic_classifiers)\n",
    "logistic_classifiers = load_model(script_path,\"logis\")\n",
    "pred_test_one_vs_all = predict_one_vs_all(logistic_classifiers, X_test, num_classes)\n",
    "print 'Test datast accuracy: %f' % (np.mean(y_test == pred_test_one_vs_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting using Softmax...\n",
      "Predicted Tags:  ['NN', 'NN', 'NN', 'NN', 'NN', 'NN', 'NN', 'NN', 'NN', 'NN']\n",
      "Predicting using OVA LR...\n",
      "Predicted Tags:  ['NN', 'NN', 'NN', 'NN', 'NN', 'NN', 'NN', 'NN', 'NN', 'NN']\n"
     ]
    }
   ],
   "source": [
    "input_sent = \"My name is Khan and I am not a terrorist\"\n",
    "input_sent = input_sent.split()\n",
    "print \"Predicting using Softmax...\"\n",
    "pred_tag_sequence(input_sent,model,soft_model,class_map,model_name=\"softmax\")\n",
    "print \"Predicting using OVA LR...\"\n",
    "pred_tag_sequence(input_sent,model,logistic_classifiers,class_map,model_name=\"logi\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "P2_B.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
